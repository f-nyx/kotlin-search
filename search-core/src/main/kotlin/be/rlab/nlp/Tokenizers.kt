package be.rlab.nlp

import be.rlab.nlp.model.Token
import org.apache.lucene.analysis.TokenStream
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute
import org.apache.lucene.analysis.tokenattributes.OffsetAttribute

/** Utilities to work with Lucene tokenizers.
 */
object Tokenizers {

    /** Executes a Lucene tokenizer and returns the result tokens.
     * @param tokenStream Lucene tokenizer stream.
     * @return the tokens generated by the tokenizer.
     */
    fun tokenize(tokenStream: TokenStream): List<Token> {
        tokenStream.use {
            val charTermAttribute: CharTermAttribute = tokenStream.addAttribute(CharTermAttribute::class.java)
            val offsetAttribute: OffsetAttribute = tokenStream.addAttribute(OffsetAttribute::class.java)
            val result: MutableList<Token> = mutableListOf()

            tokenStream.reset()

            while (tokenStream.incrementToken()) {
                result.add(
                    Token(
                        start = offsetAttribute.startOffset(),
                        end = offsetAttribute.endOffset(),
                        data = charTermAttribute.toString().toCharArray()
                    )
                )
            }

            tokenStream.end()

            return result
        }
    }
}